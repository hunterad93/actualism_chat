{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook uploads all the files in the af_knowledge_base directory to OpenAI and then creates a vector store and an assistant linked to that store.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First upload all files to OpenAI and collect their IDs, I think this could be sped up a ton by using the 'batch' feature but haven't tried that yet for file uploading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "import os\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "# Directory containing the files\n",
    "directory_path = '/Users/adamhunter/Documents/misc/actualism_chat/af_knowledge_base'\n",
    "\n",
    "# Function to upload a file to OpenAI and return the file ID\n",
    "def upload_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        response = client.files.create(\n",
    "            file=file,\n",
    "            purpose='assistants'\n",
    "        )\n",
    "        return response.id  # Access the 'id' attribute directly\n",
    "\n",
    "file_ids = []\n",
    "count = 0\n",
    "for filename in os.listdir(directory_path):\n",
    "    if count < 1300: # Count for safety... but 1300 should be enough get all the .html files\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_id = upload_file(file_path)\n",
    "            file_ids.append(file_id)\n",
    "            print(f\"Uploaded {filename} with ID {file_id}\")\n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an empty vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store first without any files\n",
    "vector_store = client.beta.vector_stores.create(\n",
    "    name=\"AF Knowledge Base Vector Store\"\n",
    ")\n",
    "vector_store_id = vector_store.id\n",
    "print(\"Vector store created with ID:\", vector_store_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect files to the vector store in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of file IDs allowed per batch\n",
    "MAX_BATCH_SIZE = 100\n",
    "\n",
    "# Function to create batches of file IDs\n",
    "def create_batches(file_ids, batch_size):\n",
    "    for i in range(0, len(file_ids), batch_size):\n",
    "        yield file_ids[i:i + batch_size]\n",
    "\n",
    "# Create and upload batches\n",
    "for batch in create_batches(file_ids, MAX_BATCH_SIZE):\n",
    "    vector_store_file_batch = client.beta.vector_stores.file_batches.create(\n",
    "        vector_store_id=vector_store_id,\n",
    "        file_ids=batch\n",
    "    )\n",
    "    print(f\"Batch created with ID: {vector_store_file_batch.id}, Status: {vector_store_file_batch.status}\")\n",
    "\n",
    "print(\"All batches have been created and are being processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an assistant linked to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assistant with the file_search tool linked to the vector store\n",
    "my_assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    name=\"AF Knowledge Base Assistant\",\n",
    "    instructions=\"\"\"You answer user queries using your knowledge base of the actual freedom trust website. You are sure to avoid filling in gaps with your own reasoning and stick to what is in the files.\"\"\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store_id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Assistant created:\")\n",
    "print(my_assistant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
