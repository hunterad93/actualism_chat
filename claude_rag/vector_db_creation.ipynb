{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to pinecone and create the index if it doesn't exist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if 'actualism-website' not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        'actualism-website',\n",
    "        dimension=1024,\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index('actualism-website').status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index('actualism-website')\n",
    "# view index stats\n",
    "index.describe_index_stats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load chunks from jsonl file, with sparse and dense vectors plus metadata including the raw text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load chunks from updated chunks jsonl file\n",
    "updated_chunks_path = 'embedded_chunks.jsonl'\n",
    "chunks = []\n",
    "with open(updated_chunks_path, 'r') as file:\n",
    "    for line in file:\n",
    "        chunks.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert sparse vector to pinecone expected format, and fill in empty ones to avoid error, should only be a small number of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sparse data to dictionary format\n",
    "def sparse_to_dict(data):\n",
    "    if data and 'indices' in data[0] and 'values' in data[0] and data[0]['indices'] and data[0]['values']:\n",
    "        return {\"indices\": data[0]['indices'], \"values\": data[0]['values']}\n",
    "    else:\n",
    "        return {\"indices\": [0], \"values\": [0.0]} # avoid pinecone error by filling in empty sparse vectors\n",
    "\n",
    "# Modify the sparse vectors in all chunks using sparse_to_dict\n",
    "for chunk in chunks:\n",
    "    if 'sparse_values' in chunk:\n",
    "        chunk['sparse_values'] = sparse_to_dict(chunk['sparse_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [09:17<00:00,  2.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 29000}},\n",
       " 'total_vector_count': 29000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 100  # Define the batch size\n",
    "index = pc.Index(\"actualism-website\")  # Connect to the correct Pinecone index\n",
    "\n",
    "# Assuming 'chunks' is your list of chunk dictionaries loaded from 'embedded_chunks.jsonl'\n",
    "total_chunks = len(chunks)  # Total number of chunks\n",
    "\n",
    "# Proceed with uploading the modified chunks to Pinecone\n",
    "for i in tqdm(range(9300, total_chunks, batch_size)):\n",
    "    i_end = min(i + batch_size, total_chunks)\n",
    "    chunks_batch = chunks[i: i_end]\n",
    "\n",
    "    upserts = []\n",
    "    for chunk in chunks_batch:\n",
    "\n",
    "        upserts.append({\n",
    "            \"id\": chunk['id'],\n",
    "            \"sparse_values\": chunk['sparse_values'],\n",
    "            \"values\": chunk['values'],\n",
    "            \"metadata\": {\n",
    "                'filename': chunk['filename'],\n",
    "                'chunk_start_index': chunk['chunk_start_index'],\n",
    "                'chunk_end_index': chunk['chunk_end_index'],\n",
    "                'raw_string': chunk['raw_string']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Upsert the modified chunks to Pinecone\n",
    "    index.upsert(upserts)\n",
    "\n",
    "# Optionally, check if the vectors have been upserted\n",
    "index.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
