{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook creates a jsonl file of text chunks with vector embeddings to upload to a vector database from a directory of .txt files, using batch processing to reduce cost of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting all documents into a series of chunks saved in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting files into chunks: 100%|██████████| 1281/1281 [00:07<00:00, 175.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 29018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_into_chunks(directory: str, encoding_name: str, chunk_size: int, overlap: int) -> list:\n",
    "    all_chunks = []\n",
    "    files = [f for f in os.listdir(directory) if f.endswith(\".md\")]\n",
    "    chunk_id = 0  # Initialize chunk ID counter\n",
    "    for filename in tqdm(files, desc=\"Splitting files into chunks\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            encoding = tiktoken.get_encoding(encoding_name)\n",
    "            tokens = encoding.encode(content)\n",
    "            start = 0\n",
    "            while start < len(tokens):\n",
    "                end = start + chunk_size\n",
    "                if end > len(tokens):\n",
    "                    end = len(tokens)\n",
    "                chunk_data = {\n",
    "                    \"id\": str(chunk_id),  # Assign the current chunk ID\n",
    "                    \"filename\": filename,\n",
    "                    \"chunk_start_index\": start,\n",
    "                    \"chunk_end_index\": end,\n",
    "                    \"raw_string\": encoding.decode(tokens[start:end]),\n",
    "                    \"values\": [],\n",
    "                    \"sparse_values\": []\n",
    "                }\n",
    "                all_chunks.append(chunk_data)\n",
    "                start += (chunk_size - overlap)\n",
    "                chunk_id += 1  # Increment chunk ID for the next chunk\n",
    "    return all_chunks\n",
    "\n",
    "# Usage\n",
    "directory = \"/Users/adamhunter/Documents/misc/actualism_chat/af_knowledge_base2\"\n",
    "chunks = split_into_chunks(directory, \"cl100k_base\", 800, 400)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin with counting the tokens in your chunks, checking the cost before you embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens in chunks: 100%|██████████| 29018/29018 [00:10<00:00, 2827.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in chunks: 22215464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def count_tokens_in_chunks(chunks: list, encoding_name: str) -> int:\n",
    "    total_tokens = 0\n",
    "    for chunk in tqdm(chunks, desc=\"Counting tokens in chunks\"):\n",
    "        raw_text = chunk['raw_string']\n",
    "        total_tokens += num_tokens_from_string(raw_text, encoding_name)\n",
    "    return total_tokens\n",
    "\n",
    "# Assuming 'chunks' is the list of chunks created from the previous function\n",
    "total_tokens = count_tokens_in_chunks(chunks, \"cl100k_base\")\n",
    "print(f\"Total tokens in chunks: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost to embed 22215464 tokens: $2.89\n"
     ]
    }
   ],
   "source": [
    "def calculate_embedding_cost(num_tokens: int, price_per_million_tokens: float, batch_processing: bool = False) -> float:\n",
    "    \"\"\"Calculate the cost of embedding a given number of tokens, with an option for batch processing.\n",
    "    \n",
    "    Args:\n",
    "    num_tokens (int): The number of tokens to be embedded.\n",
    "    price_per_million_tokens (float): The price per million tokens for embedding.\n",
    "    batch_processing (bool): If True, applies a discount for batch processing.\n",
    "    \n",
    "    Returns:\n",
    "    float: The total cost of embedding the tokens.\n",
    "    \"\"\"\n",
    "    if batch_processing:\n",
    "        price_per_million_tokens /= 2\n",
    "    cost = (num_tokens / 1_000_000) * price_per_million_tokens\n",
    "    return cost\n",
    "\n",
    "# Price per million tokens for text-embedding-3-large\n",
    "price_per_million_tokens = 0.13\n",
    "\n",
    "# Calculate the embedding cost for the total tokens with batch processing option\n",
    "batch_processing = True  # Set to True to apply batch processing discount\n",
    "embedding_cost = calculate_embedding_cost(total_tokens, price_per_million_tokens, batch_processing = False)\n",
    "print(f\"Cost to embed {total_tokens} tokens: ${embedding_cost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a sample chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'filename': 'richard---abditorium---psyche.md',\n",
       " 'chunk_start_index': 400,\n",
       " 'chunk_end_index': 1200,\n",
       " 'raw_string': ' a princess loved by Cupid; 2. soul (the immaterial essence,  animating principle, or actuating cause of an individual life; the spiritual principle embodied in human beings, all rational and spiritual  beings, or the universe); self. *(© 1994 Merriam-Webster, Inc).*  **Psychotherapy:**  \\x95 [Dictionary Definition]: \\x91psychotherapy: the treatment of disorders of emotion or personality by psychological methods;  formerly, the treatment of disease by psychic or hypnotic influence; psychotherapist: a specialist in or practitioner of psychotherapy\\x92. *(Oxford Dictionary).*    ---   **Psychic Network:** The following is the specific sense in which I use the adjective psychic in terms such as \\x91psychic network\\x92 and \\x91psychic  currents\\x92/\\x91psychic energies\\x92. Vis.:  \\x95 psychic (adj.): of or pertaining to the human mind or psyche. *(Oxford Dictionary).* \\x95 psychic (adj.): of, relating to, affecting, or influenced by the human mind or psyche. *(American  Heritage Dictionary).*  And the following is both what the word psyche refers to and its etymological derivation. Vis.:  \\x95 psyche (n.): soul, spirit, mind, fr. Latin psyche; Greek psukhe, \\x91breath, soul, life\\x92; rel. to psukhein, \\x91breathe,  blow\\x92. *(Oxford Concise Dictionary of English Etymology).*  And here is what that word \\x91soul\\x92, in the above definition, is referring to:  \\x95 soul (n.): the seat of the emotions or sentiments; the emotional part of human nature. *(Oxford  Dictionary).*  I chose to use the word soul when I first went public because, as it refers to the innermost affective entity of both those  of either a secular or spiritual persuasion (the essential difference being the materialists maintain this emotional/ passional/ intuitive self  \\x96 aka \\x91spirit\\x92[\\\\*] \\x96 dies with the body whereas the spiritualists maintain it does not), my presentation of actualism  as the third alternative to either materialism or spiritualism speaks to the self-same \\x91being\\x92, at root, with differentiation only a  connotative matter dependent upon each particular \\x91being\\x92s (occasionally changeable) partiality, or leaning, in that regard. (Incidentally, the reason why the Greek word psukhe (\\x91breath, soul, life\\x92), from which the Latin word psyche is derived,  and the related Greek word psukhein (\\x91breathe, blow\\x92) refer to breath and to breathing is because, for ancient peoples and/or primitive  peoples life began when a newly-born infant drew its first breath and ended with that body\\x92s last breath). this emotional/ passional/ intuitive self \\x96 aka \\x91spirit\\x92[\\\\*] [\\\\*]as the word spiritual means \\x91of, pertaining to, or affecting the spirit or soul\\x92, according to the Oxford Dictionary,  it too is used by those of either a secular or spiritual persuasion to refer to the self-same \\x91being\\x92, at root, with differentiation again  being a matter of a partiality/leaning connotation).   ---   **Psychosomatic**: [Dictionary Definition]: psychosomatic (adj.): of, relating to, concerned with, or involving both mind and body  [e.g.]: \\x93the psychosomatic nature of man\\x94 ~ (Herbert Ratner);  of, relating to, involving, or concerned with bodily symptoms caused by mental or emotional disturbance; \\x91psychosomatic symptoms\\x92; \\x91psychosomatic  medicine\\x92;  [e.g.]: \\x93The doctor told her that',\n",
       " 'values': [],\n",
       " 'sparse_values': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a sparse vector encoder on the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cee4dc3e0e4c15a0c701aa4a54914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29018 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "import pickle\n",
    "\n",
    "corpus = [chunk['raw_string'] for chunk in chunks]\n",
    "\n",
    "# Assuming bm25 is your pre-fit BM25Encoder object\n",
    "bm25 = BM25Encoder()\n",
    "bm25.fit(corpus)  # Fit the encoder on your corpus\n",
    "\n",
    "# Save the fitted encoder to a file\n",
    "with open('bm25_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(bm25, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    raw_text = chunk['raw_string']\n",
    "    # Encode the document as a sparse vector\n",
    "    doc_sparse_vector = bm25.encode_documents([raw_text])  # Ensure raw_text is passed as a list if required\n",
    "\n",
    "    # Since doc_sparse_vector is already in the correct format, directly add it to the chunk\n",
    "    chunk['sparse_values'] = doc_sparse_vector\n",
    "\n",
    "# Now each chunk in `chunks` has a new key 'sparse_values' containing the sparse embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save chunks to a JSONL file for backup\n",
    "with open('chunks_backup.jsonl', 'w') as file:\n",
    "    for chunk in chunks:\n",
    "        json_record = json.dumps(chunk)\n",
    "        file.write(json_record + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dense embeddings: 100%|██████████| 29018/29018 [2:39:40<00:00,  3.03it/s]  \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI()\n",
    "\n",
    "def to_dense_vector_openAI(text, client, model):\n",
    "    \"\"\"Generate a dense vector for a given text using OpenAI's embedding model.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text,\n",
    "        encoding_format=\"float\",  # Using float for direct use in Python\n",
    "        dimensions=1024\n",
    "    )\n",
    "    # Extract the dense vector values from the response\n",
    "    return response.data[0].embedding  # Assuming single text input for simplicity\n",
    "\n",
    "model = \"text-embedding-3-small\"  \n",
    "\n",
    "# Iterate over each chunk and generate a dense embedding only if 'values' is empty\n",
    "for chunk in tqdm(chunks, desc=\"Generating dense embeddings\"):\n",
    "    if not chunk['values']:  # Check if 'values' is empty\n",
    "        raw_text = chunk['raw_string']\n",
    "        dense_vector = to_dense_vector_openAI(raw_text, client, model)\n",
    "        chunk['values'] = dense_vector\n",
    "\n",
    "# Now each chunk in `chunks` that had an empty 'values' list has a new key 'values' containing the dense embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunks with dense embeddings to a JSONL file\n",
    "with open('embedded_chunks.jsonl', 'w') as file:\n",
    "    for chunk in chunks:\n",
    "        json_record = json.dumps(chunk)\n",
    "        file.write(json_record + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
